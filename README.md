# Gen AI Workshop â€“ From ANNs to GPT-2
Title: Generative AI Workshop: Exploring the Evolution from Neural Nets to LLMs
Organization: [Intellipaat]
Date: [July,2025]
Location: [Online / Bangalore, India]

# ðŸš€ Experience Overview
I recently attended a transformative workshop on Generative AI that deep-dived into the evolutionary journey from Artificial Neural Networks (ANNs) to Large Language Models (LLMs) â€” the true engines behind today's AI revolution.

# ðŸ§  Topics Covered in the Workshop:
ðŸ”¸ 1. The Evolution: ANNs â†’ RNNs â†’ Transformers â†’ LLMs
Understood the historical context and technical progression from basic feedforward networks to complex transformer-based models.

Explored the architectural breakthroughs like self-attention and positional encoding that gave rise to modern LLMs.

ðŸ”¸ 2. Hands-On with GPT-2
Worked practically with GPT-2, one of the earliest open-source transformer-based LLMs.

Observed hallucinations and weak responses, particularly due to:

Temperature settings influencing randomness and creativity.

Context window limitations and token loss issues.

GPT-2's eco-efficiency trade-offs, revealing its outdated capacity compared to newer models.

ðŸ”¸ 3. Tokenizer Internals
Explored GPT Tokenization, and how text is broken down into tokens using Byte Pair Encoding (BPE).

Analyzed how token size impacts generation, memory, and inference quality.

# ðŸ’¡ Key Learnings:
GPT-2, although foundational, struggles with precision and factual grounding.

Tuning parameters like temperature, top-k, and repetition penalty is crucial to balance creativity vs correctness.

Tokenizers are the unsung heroes in language generation pipelines â€” crucial for memory efficiency and input-output handling.

# ðŸ“Œ Tools Explored:
HuggingFace Transformers (gpt2, tokenizers)

Temperature tuning & generation strategies

Visualization of model outputs and token flows

# ðŸ”— Whatâ€™s Next?
This experience enriched my understanding of how language models evolved, and why GPT-2 stands as a critical pivot point in that journey. Iâ€™m now exploring instruction-tuned models and LLM fine-tuning for more responsible AI applications.
